{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RecSys_GNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UpFeUBE5dHke","colab_type":"code","colab":{}},"source":["# source : https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8\n","\n","!pip install torch_sparse\n","!pip install torch_scatter\n","!pip install torch_cluster\n","!pip install torch_geometric"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eW8Q2TViPnK","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","df = pd.read_csv('/content/yoochoose-clicks.data', header=None)\n","df.columns = ['session_id', 'timestamp', 'item_id', 'category']\n","\n","buy_df = pd.read_csv('/content/yoochoose-buys.data', header=None)\n","buy_df.columns = ['session_id', 'timestamp', 'item_id', 'price', 'quantity']\n","\n","item_encoder = LabelEncoder()\n","df['item_id'] = item_encoder.fit_tranform(df.item_id)\n","df_head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vIoTBOakpL9","colab_type":"code","colab":{}},"source":["# sampling the data to get smaller subset\n","\n","sampled_session_id = np.random.choice(df.session_id.unique(), 1000000, replace=False)\n","df = df.loc[df.session_id.isin(sampled_session_id)]\n","df.nunique()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQxQi0_YlbOt","colab_type":"code","colab":{}},"source":["# determining whether there is a buy event in a session\n","\n","df['label'] = df.session_id.isin(buy_df.session_id)\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuawEVDll0C4","colab_type":"code","colab":{}},"source":["# dataset construction\n","\n","# we treat each item in a session as a node \n","# all the items in the same session form a graph\n","\n","# first we group the preprocessed data by session_id\n","# for each group, the item_id is again label encoded, since for each graph node\n","# index shoud start form 0\n","\n","\n","import torch\n","from torch_geometric.data import InMemoryDataset\n","from tqdm import tqdm\n","\n","class YooChooseBinaryDataset(InMemoryDataset):\n","  def __init__(self, root, transform=None, pre_transform=None):\n","    super(YooChooseBinaryDataset, self).__init__(root, transform, pre_transform)\n","    self.data, self.slices = torch.load(self.processed_paths[0])\n","    \n","  @property\n","  def raw_file_names(self):\n","    return []\n","  \n","  @property\n","  def processed_file_names(self):\n","    return ['/content/yoochoose_click_binary_1M_sess.dataset']\n","  \n","  def download(self):\n","    pass\n","  \n","  def process(self):\n","    \n","    data_list = []\n","    \n","    # process by session_id\n","    \n","    grouped = df.groupby('session_id')\n","    for session_id, group in  tqdm(grouped):\n","      \n","      sess_item_id = LabelEncoder.fit_transform(group.item_id)\n","      group = group.reset_index(drop=True)\n","      group['sess_item_id'] = sess_item_id\n","      \n","      node_features = group.loc[group.session_id==session_id,['sess_item_id', 'item_id']\n","                               ].sort_values('sess_item_id').item_id.drop_duplicates().values\n","      node_features = torch.LongTensor(node_features).unsqueeze(1)\n","      target_nodes = group.sess_item_id.values[1:]\n","      source_nodes = group.sess_item_id.values[:-1]\n","      \n","      edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n","      x = node_features\n","      \n","      y = torch.FloatTensor([group.label.values[0]])\n","      \n","      data = Data(x=x, edge_index=edge_index, y=y)\n","      data_list.append(data)\n","      \n","    data, slices = self.collate(data_list)\n","    torch.save((data, slices), self.processed_paths[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eNURx5lz61Bl","colab_type":"code","colab":{}},"source":["# randomly shuffling the dataset\n","\n","dataset = dataset.shuffle()\n","\n","# splitting dataset into train, validation and test sets\n","\n","train_dataset = dataset[:800000]\n","val_dataset = dataset[800000:900000]\n","test_dataset = dataset[900000:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RjzeEio9WId","colab_type":"code","colab":{}},"source":["import torch\n","from torch.nn import Sequential as Seq, Linear, ReLU\n","from torch_geometric.nn import MessagePassing\n","from torch_geometric.utils import remove_self_loops, add_self_loops\n","\n","\n","class SAGEConv(MessagePassing):\n","  def __init__(self, in_channels, out_channels):\n","    super(SAGEConv, self).__init__(aggr='max')\n","    self.lin = torch.nn.Linear(in_channels, out_channels)\n","    self.act = torch.nn.ReLU()\n","    self.update_lin = torch.nn.Linear(in_channels+out_channels, in_channels, bias=False)\n","    self.update_act = torch.nn.ReLU()\n","    \n","  def forward(self, x, edge_index):\n","    # x has shape [N, in_channels] # in_channels = number of input features/activations\n","    # edge_index has shape [2, E]\n","    \n","    edge_index, _ = remove_self_loops(edge_index)\n","    edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))\n","    \n","    return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n","  \n","  def message(self, x_j):\n","    # x_j has shape [E, in_channels]\n","    \n","    x_j = self.lin(x_j)\n","    x_j = self.act(x_j)\n","    \n","    return x_j\n","  \n","  def update(self, aggr_out, x):\n","    # aggr_out has shape [N, out_channels]\n","    \n","    new_embedding = torch.cat([aggr_out, x], dim=1)\n","    new_embedding = self.update_lin(new_embedding)\n","    new_embedding = self.update_act(new_embedding)\n","    \n","    return new_embedding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6yaQ5reU7bye","colab_type":"code","colab":{}},"source":["# building GNN\n","\n","embed_dim = 128\n","\n","from torch_geometric.nn import TopKPooling\n","from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n","import torch.nn.functional as F\n","\n","class Net(torch.nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    \n","    self.conv1 = SAGEConv(embed_dim, 128)\n","    self.pool1 = TopKPooling(128, ratio=0.8)\n","    self.conv2 = SAGEConv(128, 128)\n","    self.pool2 = TooKPooling(128, ratio=0.8)\n","    self.conv3 = SAGEConv(128, 128)\n","    self.pool3 = TopKPooling(128, ratio=0.8)\n","    self.item_embedding = torch.nn.Embedding(num_embeddings=df.item_id.max()+1, embedding_dim=embed_dim)\n","    self.lin1 = torch.nn.Linear(256, 128)\n","    self.lin2 = torch.nn.Linear(128, 64)\n","    self.lin3 = torch.nn.Linear(64, 1)\n","    self.bn1 = torch.nn.BatchNorm1d(128)\n","    self.bn2 = torch.nn.BatchNorm2d(64)\n","    self.act1 = torch.nn.ReLU()\n","    self.act2 = torch.nn.ReLU()\n","    \n","  def forward(self, data):\n","    \n","    x, edge_index, batch = data.x, data.edge_index, data.batch\n","    x = self.item_embedding(x)\n","    x = x.squeeze(1)\n","    \n","    x = F.relu(self.conv1(x, edge_index))\n","    x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n","    x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n","    \n","    x = F.relu(self.conv2(x, edge_index))\n","    x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n","    x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n","    \n","    x = F.relu(self.conv3(x, edge_index))\n","    x, edge_index, _, batch, _ = self.pool3(x, edge_index, None, batch)\n","    x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n","    \n","    x = x1+x2+x3\n","    \n","    x = self.lin1(x)\n","    x = self.act1(x)\n","    x = self.lin2(x)\n","    x = self.act2(x)\n","    \n","    x = F.dropout(x, p=0.5, training=self.training)\n","    \n","    x = torch.sigmoid(self.lin3(x)).squeeze(1)\n","    \n","    return x\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUJi4cGVB8F6","colab_type":"code","colab":{}},"source":["# training GNN\n","\n","def train():\n","  \n","  model.train()\n","  \n","  loss_all = 0\n","  \n","  for data in train_loader:\n","    optimizer.zero_grad()\n","    output = model(data)\n","    label = data.y\n","    loss = crit(output, label)\n","    loss.backward()\n","    loss_all += data.num_graphs*loss_item()\n","    optimizer.step()\n","    \n","  return loss_all/len(train_dataset)\n","\n","model = Net()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n","crit = torch.nn.BCELoss() # binary cross-entropy loss\n","train_loader = DataLoader(train_dataset, batch_size=batch_size)\n","\n","for epoch in range(num_epochs):\n","  train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJkS0j0uDfF9","colab_type":"code","colab":{}},"source":["# Validation\n","\n","def evaluate(loader):\n","  \n","  model.eval()\n","  \n","  predictions = []\n","  labels = []\n","  \n","  with torch.no_grad():\n","    for data in loader:\n","      pred = model(data).detach().cpu().numpy()\n","      label = data.y.detach().cpu().numpy()\n","      predictions.append(pred)\n","      labels.append(label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzhDLtr7ETck","colab_type":"code","colab":{}},"source":["# training the model for 1 epoch\n","\n","for epoch in range(1):\n","  loss = train()\n","  train_acc = evaluate(train_loader)\n","  val_acc = evaluate(val_loader)\n","  test_acc = evaluate(test_loader)\n","  print('train_acc : ' + str(train_acc))\n","  print('val_acc : ' + str(val_acc))\n","  print('test_acc : ' + str(test_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"75-Cq_EMHvSE","colab_type":"code","colab":{}},"source":["# test\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFfzJfipH1al","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}